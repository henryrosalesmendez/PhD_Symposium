%
% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf]{acmart}



\usepackage[utf8]{inputenc}
\usepackage{fancyvrb}
\usepackage{mdframed}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{xspace}
%\usepackage[usenames, dvipsnames]{xcolor}
%\usepackage{hyperref}
\usepackage{amsmath}
\newcommand{\argmin}{\arg\!\min}
\newcommand{\argmax}{\arg\!\max}

%para el simbolo de chequeado
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\usepackage{booktabs} 
\usepackage{multirow}

\newcommand{\ah}[1]{{\color{blue}\textsc{ah:} #1}}

\usepackage{soul} %middleline
\usepackage{pgfplots}




%
% defining the \BibTeX command - from Oren Patashnik's original BibTeX documentation.
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08emT\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
% Rights management information. 
% This information is sent to you when you complete the rights form.
% These commands have SAMPLE values in them; it is your responsibility as an author to replace
% the commands and values with those provided to you when you complete the rights form.
%
% These commands are for a PROCEEDINGS abstract or paper.
\copyrightyear{2018}
\acmYear{2018}
\setcopyright{acmlicensed}
\acmConference[PhD Track-WWW 2019]{PhD Symposium session of The Web Conference 2019}{May 13--17, 2019}{San Francisco, USA}
\acmBooktitle{PhD Symposium session of The Web Conference 2019, May 13--17, San Francisco, USA}
\acmPrice{15.00}
\acmDOI{10.1145/1122445.1122456}
\acmISBN{978-1-4503-9999-9/18/06}

%
% These commands are for a JOURNAL article.
%\setcopyright{acmcopyright}
%\acmJournal{TOG}
%\acmYear{2018}\acmVolume{37}\acmNumber{4}\acmArticle{111}\acmMonth{8}
%\acmDOI{10.1145/1122445.1122456}

%
% Submission ID. 
% Use this when submitting an article to a sponsored event. You'll receive a unique submission ID from the organizers
% of the event, and this ID should be used as the parameter to this command.
%\acmSubmissionID{123-A56-BU3}

%
% The majority of ACM publications use numbered citations and references. If you are preparing content for an event
% sponsored by ACM SIGGRAPH, you must use the "author year" style of citations and references. Uncommenting
% the next command will enable that style.
%\citestyle{acmauthoryear}
%\pgfplotsset{compat=1.15}
%
% end of the preamble, start of the body of the document source.
\begin{document}

%
% The "title" command has an optional parameter, allowing the author to define a "short title" to be used in page headers.
\title{Towards Better Entity Linking Evaluation}

%
% The "author" command and its associated commands are used to define the authors and their affiliations.
% Of note is the shared affiliation of the first two authors, and the "authornote" and "authornotemark" commands
% used to denote shared contribution to the research.
\author{Henry Rosales-M\'endez}
\affiliation{%
  \institution{DCC, University of Chile}
}
\email{hrosales@dcc.uchile.cl}




%
% By default, the full list of authors will be used in the page headers. Often, this list is too long, and will overlap
% other information printed in the page headers. This command allows the author to define a more concise list
% of authors' names for this purpose.
\renewcommand{\shortauthors}{Rosales-M\'endez et al.}

%
% The abstract is a short summary of the work to be presented in the article.
\begin{abstract}
The Entity Linking (EL) task is concerned with linking entity mentions in a text collection with their corresponding knowledge-base entries. Despite the progress made in the evaluation of EL systems, there is still much work to be done, where this Ph.D. research tackles issues concerning EL evaluation. Among these issues, we stress (a)~the lack of datasets that allow for cross-language comparison, (b)~the lack of consensus about the definition of ``entity'', (c)~the lack of evaluation metrics that allow for different notions of entities, and (d)~the focus on evaluating high-level systems rather than low-level techniques. By addressing these challenges and better understanding the performance of EL systems, our hypothesis is that we can create a more general, more configurable EL framework that can be better adapted to the needs of a particular application. In the early stages of this PhD work, we have identified these problems and begun to address (a--c), publishing initial results that constitute a significant step forward in our investigation. However, there are still further challenges that must be addressed before we reach our goal. Our next steps thus involve proposing a more fluid definition of ``entity'' adaptable to different applications, the definition of quality measures that allow for comparing EL approaches targeting different types of entities, as well as the creation of a customizable EL framework that allows for composing and evaluating individual techniques as appropriate to a particular task.
\end{abstract}

%
% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
% Please copy and paste the code instead of the example below.
%
\begin{comment}
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}

%
% Keywords. The author(s) should pick words that accurately describe the work being
% presented. Separate the keywords with commas.
\keywords{datasets, neural networks, gaze detection, text tagging}

%
% A "teaser" image appears between the author and affiliation information and the body 
% of the document, and typically spans the page. 

\begin{teaserfigure}
  \includegraphics[width=\textwidth]{sampleteaser}
  \caption{Seattle Mariners at Spring Training, 2010.}
  \Description{Enjoying the baseball game from the third-base seats. Ichiro Suzuki preparing to bat.}
  \label{fig:teaser}
\end{teaserfigure}
\end{comment}
%
% This command processes the author and affiliation and title information and builds
% the first part of the formatted document.
\maketitle

%------------------------------------------------------------
\section{Problem}

Entity Linking (EL) is a task in Information Extraction (IE) that focuses on linking the entity mentions in a text collection with entity identifiers in a given Knowledge Base (KB). Such a task has various applications, including semantic search, document classification, semantic annotation, and text enrichment, as well as forming the basis for further IE processes. Despite the fact that works addressing the EL tasks have been pursued by various communities and published in various international conferences, some fundamental questions remain open regarding the aim of the task and how EL results should be evaluated.

First and foremost, despite the presence of various gold standard datasets, evaluation frameworks, etc., it is still unclear what EL systems should link. There is evidence of disagreement in the EL community on this matter, and as a consequence, different systems target different types of entities. This phenomenon is illustrated in Figure~1, which contains the results for a short example text of four state-of-the-art systems that are popular in the community: Babelfy~\cite{0001RN14}, DBpedia Spotlight~\cite{MendesJGB11}, TagMe~\cite{FerraginaS10} and FRED~\cite{GangemiPRNDM17}. As we can observe in Figure~\ref{fig:exp1}, there are signs of fundamental disagreements among the involved systems. While FRED and Babelfy consider only proper names, TagME and DBpedia Spotlight also include other nouns for which a corresponding KB entity exists and which do not constitute names. Furthermore, overlapping mentions (denoted by ``\{\}'') are targeted by Babelfy and TagME, but not by DBpedia Spotlight nor FRED. So which system is more correct?

\begin{figure}[!tb]
\begin{mdframed}
A [second]$^{t}$, larger and more [theatrical]$^{t}$ [Cirque]$^{t}$ [show]$^{t}$, [\{Michael Jackson\}$^{btdf}$: One]$^{bt}$, designed for [residency]$^{t}$ at the [Mandalay Bay]$^{btdf}$ [resort]$^{d}$ in [Las \{Vegas\}$^{b}$]$^{td}$.
\end{mdframed}
\caption{Output annotations of Babelfy (b), TagME (t), DBpedia Spotlight (d) and FRED (f) over the same input \label{fig:exp1}}
\end{figure}

This lack of consensus affects further processing of EL systems' outputs since different application scenarios have different requirements on what mentions should be involved. Furthermore, this problem also complicates EL assessment because we do not know how we can define the ideal result that such a system should achieve. Some efforts have been made to standardize which mentions we should identify for annotation, as is the case of the work by Jha et al.~\cite{JhaRN17}, who propose a set of rules to serve as guidelines for benchmark creation. However, these rules force the adoption of some considerations that may not suit certain applications and on which there is thus no consensus. For instance, Jha et al., advocate for the omission of overlapping mentions like ``\{Michael Jackson\}'', but authors such as Ling et al.~\cite{LingSW15} disagree. In a semantic search scenario, for example, looking at Figure~\ref{fig:exp1}, should such a document be considered relevant for a user interested in texts about Michael Jackson, or more generally, texts about American pop singers?

Several EL benchmark datasets have been proposed that -- although used by a variety of systems -- also exhibit this disagreement. While KORE50~\cite{HoffartSNTW12} only annotates proper names, the DBpedia Spotlight dataset~\cite{MendesJGB11} includes annotations of common nouns such as \textit{software} and \textit{owner}. Additionally, the DBpedia Spotlight dataset includes overlaps, for example, ``Google car'' is linked with \texttt{dbr:Google\_self-driving\_car}\footnote{Throughout, we use well-known prefixes according to \url{http://prefix.cc}}, while `car' is linked to \texttt{dbr:Car}. 

The solution thus far to address the lack of a consensus has been to define a new consensus, but we propose that a different approach is needed: that no one size fits all in terms of EL. This should not only be reflected in EL systems, but also in EL datasets and metrics used to evaluate EL systems with respect to such datasets. This presents a major challenge tackled in the context of this PhD work.

Such disagreement on what EL systems should link is not the only issue we have encountered in terms of evaluating the EL task. Another major issue is that despite some recent developments for other languages, most work has focused on English texts, both in terms of EL systems and EL datasets. Focusing on a multilingual context, some authors have proposed approaches with a large list of languages. For instance, this is the case of MAG~\cite{MoussallemURN18}, a multilingual EL system that supports annotations over 40 languages. However, the presence of such systems raises new challenges for evaluating the EL task. Such questions now include: How well do EL systems perform outside of English as a primary language? Do multilingual approaches behave equally for all of their supported languages? If not, why not? Are multilingual EL approaches really necessary with recent improvements in machine translation? These questions are not deeply studied yet in the literature. Indeed, only a few of the current EL datasets are multilingual, which complicate any kind of multilingual experimentation. 

Generalizing these issues, different applications for EL may bring with them different requirements, which may be best addressed using different techniques. Aside from the issue of the types of entities and the languages targeted, there are also issues regarding for example the length of the text(s), the noise present, the domain of a text, the need to perform EL on semi-structured inputs (e.g., HTML), etc. Different EL systems proposed in the literature have been proposed to solve individual tasks. But individual systems may package together a specific set of techniques, where evaluation is conducted at the level of systems (or ensembles of systems) without understanding which techniques work best in which scenarios. Our ultimate goal, then, is to develop a EL framework that allows for composing and evaluating individual EL techniques, allowing to find the configuration best suited to a particular setting.



%In some works, EL is only considered to refer to the ED phase~\cite{Plu0T16}. Here we see EL as being composed of both ED and ER.
%-----------------------------------------------------------------------------
\section{Background}
Entity Linking is a task in Information Extraction that focuses on linking the entity mentions in a text collection with entity identifiers in a given knowledge base. Formally, let $E$ be a set of entities in a KB and $M$ the set of entity mentions in a given text collection. The EL process focuses on linking each entity mention $m\in{}M$ in a text collection with an entity identifier $e\in{}E$ in a given Knowledge Base (KB). Nowadays, there are large KBs that describe a huge list of entities (such as Wikipedia, DBpedia, Wikidata, etc.); furthermore, new entities emerge every day. Those mentions not (yet) included in the KB are labeled \textit{NIL} (Not In Lexicon).

Generally speaking, EL models are commonly separated into two main phases, detailed below:
\begin{description}
\item[Entity Recognition (ER)] This phase spots which phrases of the input text should be taken as mentions. This problem is also addressed by the Named Entity Recognition (NER) task, where a variety of techniques have been employed to this goal. On the other hand, some works regard ER itself as an independent task, out of the scope of EL~\cite{Plu0T16}. 

\item[Entity Disambiguation (ED)] This phase decides which KB entities should be associated with the identified mentions. This phase is commonly divided into the following steps:
\begin{description}
\item \textit{Candidate entity generation}:
For each entity mention $m\in{}M$ this stage selects $E_m$: a candidate set $E_m \subseteq E$ that represents entities with a high probability of corresponding to $m$ is selected. Often this selection is based on matching $m$ with entity labels for $E$ in the knowledge base.

\item \textit{Candidate entity ranking}:
Each entity $e_m \in E_m$ is ranked according to an estimated confidence that it is the referent of the textual mention $m$. This can be performed considering a variety of features, such as the perceived ``popularity'' of $e_m$, its relation to candidates for nearby mentions, and so forth. The candidate in $E_m$ with the best ranking may be selected as the link for $m$, possibly assuming it meets a certain threshold confidence (or other criteria). 

\item \textit{Unlinkable mention prediction}:
Some tools consider unlinkable mentions, where no entity in the knowledge base meets the required confidence for a match to a given entity mention $m$. Depending on the application scenario, these mentions may be simply ignored, or may be proposed as ``emerging entities'' -- annotated as NIL -- that could be added to the knowledge base in the future.
\end{description}
\end{description}
In some more recent EL systems, the division between the EL and ED phases is less clear. Some systems apply an End-to-End approach, while other systems apply ER and EL jointly in the same model in the goal of optimizing for both tasks in one process~\cite{WangLWC12,NguyenTW16}. Other systems assume that entity mentions have already been identified by an existing ER approach and specifically address ED~\cite{Plu0T16}. 

Several EL approaches have been proposed in the literature. Some of them take annotations, KB entities and their relationship as a graph and perform heuristic to find the proper matching. For instance, Babelfy~\cite{0001RN14} and AIDA~\cite{HoffartYBFPSTTW11} search the densest sub-graph applying a Random Walk with Restart and a greedy algorithm respectively. Other approaches as TagME~\cite{FerraginaS10} , THD~\cite{DojchinovskiK13}, DBpedia Spotlight~\cite{MendesJGB11} and FREME~\cite{SasakiDN16} are based on similarity functions between mentions and the KB content. For instance, TagME ranks the candidate entities by two functions: \textit{commonness} and \textit{relatedness}, the first count how frequently an anchor text is linked to a particular Wikipedia entity and the second, returns how often candidate entities for different mentions are annotated from the same Wikipedia page. On the other hand, WIKIME~\cite{TsaiR16} uses multilingual embedding which is trained for words and Wikipedia titles. 

%--------------------------------------------------------------------
\section{PROPOSED RESEARCH}

The present Ph.D. work proposes to address a variety of open questions regarding the evaluation of EL systems. In this direction, the following research questions are being or will be addressed:

\begin{enumerate}
    \item What should Entity Linking link?
    \begin{enumerate}
        \item How can we define the goal of the EL task?
        \item Is consensus possible on the definition of an ``entity''?
        \item If not, how can we define benchmark EL datasets and what metrics can we use to reflect the lack of consensus?
        %\item[b)] Why more than one entity definition?
    \end{enumerate}
    
    \item How well do EL systems perform in multilingual settings?
    \begin{enumerate}
    	\item How can we compare EL performance across langauges?
        \item How do EL systems perform for different languages?
        \item Why do results differ across languages?
        \item Are multilingual EL approaches necessary with recent improvement in machine translation? 
    \end{enumerate}
    
    \item How can we adapt and configure EL techniques for different applications?
    \begin{enumerate}
    	\item What are the specific applications for EL?
    	\item What are the different settings that can be considered?
    	\item Which techniques work best under what assumptions?
    	\item How can existing EL techniques be best configured to meet the needs of a particular application setting?
    \end{enumerate}
\end{enumerate}

We now discuss these three high-level research questions in more detail, describing the issues faced, as well as the ongoing work and plans for future work to address them.

\subsection{Lack of consensus}

The concept of ``named entity'' was first coined by the 6th Message Understanding Conference~\cite{GrishmanS96} (MUC-6) where entities are assigned to one of the classes \textit{Person}, \textit{Location}, \textit{Organization} and other numeric/temporal expressions. Hence the definition of an entity follows from these classes: any instance of such a class is considered an entity, and per the consensus of MUC-6, the goal of NER is to identify mentions of entities of one of these classes.

With the advent of large-scale, diverse KBs, interest grew in the EL task, which not only identifies (and types) entities in a text, but also links them to the KB. Many ER models and ER benchmark datasets built upon the extensive work in the NER community, proposing to recognize the same entities from the same classes but additionally link them to the KB. This perspective is inherited by many EL systems which continue to identify mentions with NER tools. However, the KBs to which EL systems link often contain classes not considered in the MUC-6 consensus. 

Hence EL systems began to develop custom ER techniques that target a broader range of entity types. For instance, in Figure~\ref{fig:exp1}, ``Michael Jackson'' would belong to the MUC-6 class \textit{Person}, whereas ``Michael Jackson: One'' -- though present as an entity in KBs such as DBpedia and Wikipedia -- would be excluded by MUC-6, referring to a theatrical production. Along these lines, some authors chose to extend the initial MUC-6 classes, including also \textit{Products}, \textit{Financial Entities}~\cite{MinardSUAESS16}, \textit{Films}, \textit{Scientists}~\cite{EtzioniCDPSSWY05}, etc. On the other hand, other authors propose to separate current classes to more specific ones, for instance, deriving \textit{City}, \textit{State}, \textit{Country} from the class \textit{Location}~\cite{Fleischman01}. Generally speaking, however, 
class-based definitions of entities are inflexible as they cannot hope to adapt to the variety of types present in large KBs. For instance, Wikidata alone has entities from 50,000 unique classes. Therefore, other authors advocate for more general definitions, but these often lack formality~\cite{EckhardtHPS14, UrenCIHVMC06}. One option is to use a \textit{Miscellaneous} class of entities, but this leaves the question of what sorts of entities this class should cover. Other authors have tried to provide a more general definition of entity, such as the definition ``\textit{substrings corresponding to world entities}'' used by Ling et al.~\cite{LingSW15}; however, such a definition is cyclical, due to using the word ``\textit{entity}'' in the definition of an ``\textit{entity}''. 

\paragraph{Proposal:} Instead of addressing the abstract question ``\textit{what is an `entity'?}'', our position is to rather address the more practical question ``\textit{what should Entity Linking link?}''. Posed this way, the question suggests a practical response: ``\textit{it depends on the application!}''~\cite{ourAMW2018}. For example, in a semantic search scenario, where the goal is to find documents mentioning particular entities or particular types of entities, finding all repeated mentions of an entity may not be so key a requirement for EL: finding any mention in the document might suffice. On the other hand, for relation extraction, each mention might refer to a potential relation in the text, and hence the requirements for EL change. With this in mind, our goals are to first understand on which types of entities there is consensus in the community, and on which not. Then we wish to better understand what are the applications for EL, and how the choice of application affects the requirements of the EL system. Finally we aim to develop EL benchmarks and metrics that -- rather than assuming a one-size-fits-all definition of an entity -- reflect our findings in terms of varying consensus, applications and requirements.

%---------------------------------------------------------------------
\subsection{Multilingual EL}


\newcommand{\ccell}[1]{\multicolumn{1}{c}{#1}}
%\setlength{\tabcolsep}{1.2ex}
\begin{table}[tb!]
\centering

\caption{Survey of popular EL datasets; for multilingual datasets, the quantities shown refer to the English data available. We present metadata about the relaxed and strict version of our dataset by \textsc{VoxEL}$_R$ and \textsc{VoxEL}$_S$ respectively.}
\label{tab:datasets} 
%\resizebox{\textwidth}{!}{
\begin{tabular}{lc}
\toprule
\textbf{Dataset}~~~~~~~~~~~~~~~~~~ & \ccell{\textbf{Languages}}\\\midrule
AIDA/CoNLL-Complete~\cite{aida2011}&EN \\
KORE50~\cite{kore50}                &EN \\
IITB~\cite{IITB2009}                &EN \\
ACE2004~\cite{aquaint}              &EN\\
AQUAINT~\cite{aquaint}              &EN \\
MSNBC~\cite{cucerzan2007large}      &EN \\
DBpedia Spotlight
\cite{mendes2011dbpedia}            &EN \\
N3-RSS 500~\cite{n3}                &EN \\
Reuters 128~\cite{n3}               &EN \\
Wes2015~\cite{wes2015}              &EN \\\midrule
News-100~\cite{n3}                  &DE \\
Thibaudet~\cite{renden2016}         &FR \\
Bergson~\cite{renden2016}           &FR \\\midrule
SemEval 2015 
Task 13~\cite{moro2015semeval}      &EN,ES,IT \\
DBpedia Abstracts
~\cite{abstracts2016}               &DE,EN,ES,FR,IT,JA,NL \\
MEANTIME \cite{meantime2016}        &EN,ES,IT,NL \\
VoxEL$_R$                           &DE,EN,ES,FR,IT\\  
VoxEL$_S$                           &DE,EN,ES,FR,IT\\ 
\bottomrule
\end{tabular}
%}
\end{table}

Thus far, the bulk of effort in EL research has been devoted to English texts. However, more recently, a number of multilingual EL systems -- supporting multiple languages -- have been proposed. Such systems raise new questions for evaluating EL: How well would state-of-the-art approaches perform over non-English corpora? How would performance vary across languages (and why)? Given that EL often targets named entities, how important is it for EL systems to be configurable for different languages (e.g., \textit{Michael Jackson}'s name does not change with language, only with alphabet)? Given recent improvements in machine translation, how do multilingual EL systems perform versus translating input text?

One challenge faced for responding to these questions is the short list of multilingual datasets available that could be used to evaluate EL performance for various languages. In Table~\ref{tab:datasets} we survey a variety of EL datasets available in the literature, where we can observe in Table~\ref{tab:datasets} that the majority only consider English text; others that consider non-English texts only offer one language. While a number of multilingual datasets have now been made available, they further present some issues for comparing EL systems across languages, as we will discuss later.


\paragraph{Initial Results:} Taking an existing multilingual dataset -- SemEval 2015 Task 13 -- in~\cite{Rosales-MendezP17}, we perform initial experiments to compare the performance of popular EL systems for English and Spanish texts, testing Babelfy, DBpedia-Spotlight, WikiMe, TagME, THD and AIDA. Only the first three of these systems can be explicitly configured for Spanish texts. Table~\ref{tab:evaluateEL} offers an overview of the main results. As can be observed, systems generally perform considerably better for English than Spanish, particularly (but not limited to) EL systems not configurable for Spanish. We consider this result as being potentially due to three main factors (a) KBs (e.g., Wikipedia) contains different information for both languages with potentially more information available in English, (b) the models/techniques change according to the target language where, for example, DBpedia-Spotlight's ER uses different models according to the targeted language, and (c) the presence of variations in the languages themselves, where, for example, recognizing ``\textit{Star Wars}'' is less challenging than the Spanish version ``\textit{La guerra de las galaxias}'' due to capitalization rules in Spanish and the phrase length.

%In this experiment we run each system twice for each language, initially, we take as input the sentences, and next to whole document. In this way, we can observe their quality from two different points of view, which both are presented in real environments. While some systems are proposed to deal with short text, (e.g., TagMe), the general behavior if the involved approaches are not substantially different. 

\begin{table}[tb!]
\centering
\caption{Overall EL evaluation ($F_1$) of selected approaches for the SemEval 2015 Task 13 in Spanish (ES) and English (EN). Approaches configured for Spanish are italicized.}
\label{tab:evaluateEL}
\begin{tabular}{lrr}
\toprule
\textbf{System} &~~~\textbf{ES}~~~&~~~\textbf{EN}~~~ \\ \midrule
%\multicolumn{3}{c}{\textit{Sentence level}} \\ \midrule
%\textit{Babelfy}          &0.420&0.568\\
%\textit{DBpedia-Spotlight}&0.364&0.414\\
%\textit{WikiMe}           &0.033&0.051\\
%TAGME                     &0.179&0.463\\
%THD                       &0.069&0.120\\
%AIDA                      &0.010&0.044\\
%\midrule
%\multicolumn{3}{c}{\textit{Document level}} \\ \midrule
\textit{Babelfy}          &0.439&0.602\\
\textit{DBpedia-Spotlight}~~~~&0.337&0.414\\
\textit{WikiMe}           &0.043&0.043\\ \midrule
TAGME                     &0.133&0.395\\
THD                       &0.069&0.110\\
AIDA                      &0.010&0.046\\
\bottomrule
\end{tabular}
\end{table}





\paragraph{Further Results:} One of the obstacles to ongoing research on multilingual EL is the low availability of datasets with the same text in different languages. According to our survey in Table~\ref{tab:evaluateEL}, there are only three multilingual datasets available (the VoxEL datasets are proposed by us). As we detected in the initial work described previously, each of the three datasets has its own limitations. SemEval 2015 Task 13 is composed of four documents on biomedical, math, computer and social topics; DBpedia Abstracts\footnote{\url{http://wiki-link.nlp2rdf.org/abstracts/}; January 1st, 2018} is a large corpus build automatically from the abstracts (first paragraph) the Wikipedia pages, containing in total 39132 documents; and MEANTIME contains annotations of 120 news articles from WikiNews\footnote{\url{https://en.wikinews.org/}; January 1st, 2018} with annotations of entities, events, temporal information and semantic roles. However, DBpedia Abstracts is not a parallel corpus: the text differs across languages, making it unsuitable for comparing performance across languages. On the other hand, while SemEval 2015 Task 13 and MEANTIME aim to be parallel corpora, they have different annotations in different languages. Hence these datasets are not ideal for comparing the performance of EL systems across different languages (though they can be used for comparing EL systems across individual languages). Furthermore, these datasets adopt a particular notion of entity, which as argued previously, may not be that agreed upon by the community.

In order to better compare EL performance across multiple languages, we proposed a new multilingual corpus called VoxEL~\cite{VoxEL2018} with the aim of ensuring the same annotations across different languages, as well as reflecting in the dataset the lack of consensus on what is an entity. VoxEL is based on 15 news articles from the European newsletter VoxEurop\footnote{http://www.voxeurop.eu/; January 1st, 2018}, which is translated by professionals to different European languages. We first aligned the sentences and entities across languages, resolving cases where some entities and sentences were omitted/changed in the translations. To address the lack of consensus about what is an entity, we include two versions of the same dataset: one \textit{strict} version that includes only those entities that all systems appear to agree should be linked (i.e., \textit{Person}, \textit{Location} and \textit{Organization}), and another \textit{relaxed} version that includes also all mentions (including overlapping mentions) with a Wikipedia page related to them (e.g., `software',  `resort', etc.). Our results again show that -- aside from Babelfy -- EL systems generally perform much better over English texts. We also compare in~\cite{Rosales-MendezH18a} the idea of using machine translation in EL environments to translate the input text rather than configuring the EL system for the native text. The results show that the majority of systems -- namely DBpedia Spotlight, FREME and TagMe -- perform better when the input text is either in English, or translated to English. %On the other hand, Babelfy commonly obtains better results for (translated) Spanish texts in the Relaxed version, and (translated) Italian texts in the Strict version. 


\subsection{Configurable EL Framework}

Our general hypothesis in this Ph.D. work is that when it comes to EL systems, one size does not fit all: different scenarios and different applications may have different requirements for an EL system, including, but not limited to, the types of entities targeted, the languages supported, etc. Drawing the Ph.D. work together, our goal is to be able to perform finer-grained evaluation of EL systems under different requirements and different assumptions. Going one step further, rather than evaluating EL systems, we wish to be able to evaluate the effects of different ER and ED techniques. A given EL system already configures a number of techniques into one solution, where the results of a system then confound the performance of these techniques. While some EL papers present results for ER and ED tasks separately, we propose to go one step further.


\paragraph{Proposal:} We propose to create a modular EL framework that implements a selection of the most important EL techniques found in the literature. Such a framework should allow for creating custom EL pipelines that allow for evaluating and composing techniques according to the requirements of a particular application. The challenges posed by this proposal are various and include not only the engineering challenge of developing such a modular framework but also some non-trivial research questions. First we must address the question of how the requirements of a particular application can be represented. Second we must consider what datasets and metrics can be used to evaluate individual EL techniques (and their composition) per these requirements. Though ambitious, if successful, this stage of the Ph.D.\ work could lead to a better understanding of how EL techniques perform under different assumptions and further offer the practical contribution of a generalized EL framework adaptable to a broader variety of applications and settings.

%---------------------------------------------------------------------
\section{Methodology}

Despite the formulation and addressing of this issues constitute a step forward in our investigation, our next steps are aimed to the proposition of a universal definition of ``entity'' that could integrate all the work done, and well as a quality measure that allow the comparison of approaches that are targeting a different set of entities. In this direction, our further work can be outlined through the following tasks:

\begin{enumerate}
    \item Review the state-of-the-art of current quality measures. Despite that many quality measures have been adopted in El from related areas (e.g., precision, recall, F$_1$, Accuracy), none of them take into account the differentiation of strict and relaxed annotations. 
    
    \item Propose a categorization of mention that allows the inclusion of current entity definitions, rather than only separate them on strict and relaxed.
    \item Design and Build an EL architecture focused on multilingual environments.
    \item Implement a general machine-learning framework based on the aforementioned architectures that allow us to combine features according to the application scenario.
    
    
\end{enumerate}


%----------------------------------------------------------------------------
\section{Conclusion}

As a phD proposal, we plan to develop a general machine-learning EL framework that allows us to overcome state-of-the-art approaches, specifically dealing with multilingual context. However, this ongoing research has been complicated due to several problems that difficult the progress. For instance, the lack of consensus about what should entity linking link, the scarcity of annotated datasets with the same text in different languages, and the lack of criteria of current quality measures to distinguish strict and relaxed annotations according to the different domains. 

For this reason, in this paper we show what we are done to overcome this inconvenient. We stress that the definition of entity should be associated with the domain of application, as well as a new multilingual dataset that contains the same document, sentences and annotations coss-language. Du to is datasets properties, we use it to figure out that is better to translate text -- by machine translation -- and it as input for some EL approaches, than applied their own multilingual model. As future work, we plan to propose a quality measure that distinguish between strict and relaxed annotations. 

%----------------------------------------------------------------------------
\section{Acknowledgments}
I want to thank my advisors Aidan Hogan ad Barbara Poblete for supervising my PhD research. This work was supported by CONICYT-PCHA/Doctorado Nacional/2016-21160017 and by the Millennium Institute for Foundational Research on Data (IMFD).

%
% The next two lines define the bibliography style to be used, and the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{bibfile}

\end{document}
